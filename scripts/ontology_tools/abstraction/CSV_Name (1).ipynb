{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeznM78tmiqw",
        "outputId": "8a4bff4f-76a3-4c5c-f23f-8aa666fa1af0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/trial.csv', '/content/Context')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import re\n",
        "\n",
        "# === Step 1: Unzip ontology files ===\n",
        "zip_path = \"/content/family_1hop_no_tbox.zip\"\n",
        "extract_dir = \"/content/family_1hop_no_tbox/family_1hop_no_tbox\"\n",
        "\n",
        "if not os.path.exists(extract_dir):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "\n",
        "# === Step 2: Load the CSV ===\n",
        "csv_path = \"/content/SPARQL_questions_sampling (3).csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Ensure output directory exists\n",
        "context_dir = \"/content/Context\"\n",
        "os.makedirs(context_dir, exist_ok=True)\n",
        "\n",
        "# === Helper Functions ===\n",
        "\n",
        "def extract_individual_names(ontology_text, properties):\n",
        "    \"\"\"\n",
        "    Find all individuals in the ontology by:\n",
        "     1) looking for triples of the form ns1:name1 ns1:prop ns1:name2 .\n",
        "     2) looking for NamedIndividual blocks (ns1:name a owl:NamedIndividual ; …).\n",
        "\n",
        "    Returns a list of unique names in order of first appearance.\n",
        "    \"\"\"\n",
        "    triple_pattern = re.compile(\n",
        "        r'ns1:([a-zA-Z0-9_]+)\\s+ns1:(' + '|'.join(properties) + r')\\s+ns1:([a-zA-Z0-9_]+)\\s*\\.'\n",
        "    )\n",
        "    block_pattern = re.compile(\n",
        "        r'ns1:([a-zA-Z0-9_]+)\\s+a\\s+owl:NamedIndividual\\s*;\\s*((?:.|\\n)*?)(?=\\.\\s)',\n",
        "        re.MULTILINE\n",
        "    )\n",
        "\n",
        "    name_set = set()\n",
        "    name_list = []\n",
        "\n",
        "    # 1) Triples\n",
        "    for match in re.finditer(triple_pattern, ontology_text):\n",
        "        subj, _, obj = match.groups()\n",
        "        for name in (subj, obj):\n",
        "            if name not in name_set:\n",
        "                name_list.append(name)\n",
        "                name_set.add(name)\n",
        "\n",
        "    # 2) NamedIndividual blocks\n",
        "    for block_match in re.finditer(block_pattern, ontology_text):\n",
        "        subject = block_match.group(1)\n",
        "        if subject not in name_set:\n",
        "            name_list.append(subject)\n",
        "            name_set.add(subject)\n",
        "\n",
        "        # Within that block, find any ns1:OtherName occurrences\n",
        "        # Format inside block can be: ns1:prop ns1:name ; ...\n",
        "        inner_text = block_match.group(2)\n",
        "        related = re.findall(r'ns1:([a-zA-Z0-9_]+)', inner_text)\n",
        "        for obj in related:\n",
        "            if obj not in name_set:\n",
        "                name_list.append(obj)\n",
        "                name_set.add(obj)\n",
        "\n",
        "    return name_list\n",
        "\n",
        "def replace_in_sparql(sparql_text, mapping):\n",
        "    \"\"\"\n",
        "    Given a SPARQL query string (with URIs like <...#name>),\n",
        "    replace every occurrence of '#name' → '#IndividualX' based on mapping.\n",
        "    \"\"\"\n",
        "    result = sparql_text\n",
        "    for old_name, new_id in mapping.items():\n",
        "        # Match \"#old_name\" as a whole word\n",
        "        pattern = rf\"#\\b{re.escape(old_name)}\\b\"\n",
        "        result = re.sub(pattern, f\"#{new_id}\", result)\n",
        "    return result\n",
        "\n",
        "def replace_all_ns1_names(text, mapping):\n",
        "    \"\"\"\n",
        "    Replace ns1:<old> → ns1:<new> throughout the ontology text.\n",
        "    \"\"\"\n",
        "    for old, new in mapping.items():\n",
        "        text = re.sub(fr'\\bns1:{re.escape(old)}\\b', f'ns1:{new}', text)\n",
        "    return text\n",
        "\n",
        "# === Step 3: Process each Root Entity separately ===\n",
        "\n",
        "properties = [\n",
        "    \"hasBrother\", \"hasChild\", \"hasDaughter\", \"hasSon\", \"hasPartner\", \"hasFemalePartner\",\n",
        "    \"hasMalePartner\", \"hasRelation\", \"hasAncestor\", \"hasParent\", \"hasFather\", \"hasMother\",\n",
        "    \"isBloodrelationOf\", \"isSiblingOf\", \"isBrotherOf\", \"isSisterOf\", \"hasSex\", \"hasSister\",\n",
        "    \"hasSpouse\", \"hasHusband\", \"hasWife\", \"isAncestorOf\", \"isChildOf\", \"isDaughterOf\",\n",
        "    \"isSonOf\", \"isFatherOf\", \"isFemalePartnerln\", \"isHusbandOf\", \"isMalePartnerln\",\n",
        "    \"isMotherOf\", \"isParentOf\", \"isPartnerln\", \"isSpouseOf\", \"isUncleOf\", \"isWifeOf\"\n",
        "]\n",
        "\n",
        "new_DA_answers = []\n",
        "new_DA_sparqls = []\n",
        "\n",
        "unique_entities = df['Root Entity'].unique()\n",
        "seen_entities = set()\n",
        "\n",
        "for entity in unique_entities:\n",
        "    entity_rows = df[df['Root Entity'] == entity].copy()\n",
        "\n",
        "    if entity in seen_entities:\n",
        "        # We'll still need to append placeholders for those rows\n",
        "        for _ in entity_rows.itertuples():\n",
        "            new_DA_answers.append(\"\")\n",
        "            new_DA_sparqls.append(\"\")\n",
        "        continue\n",
        "\n",
        "    seen_entities.add(entity)\n",
        "    ttl_path = os.path.join(extract_dir, entity + \".ttl\")\n",
        "    if not os.path.exists(ttl_path):\n",
        "        print(f\"Missing TTL: {ttl_path}\")\n",
        "        for _ in entity_rows.itertuples():\n",
        "            new_DA_answers.append(\"\")\n",
        "            new_DA_sparqls.append(\"\")\n",
        "        continue\n",
        "\n",
        "    with open(ttl_path, \"r\") as f:\n",
        "        ontology_text = f.read()\n",
        "\n",
        "    # 3a) Extract individuals and build mapping: name → IndividualX\n",
        "    name_list = extract_individual_names(ontology_text, properties)\n",
        "    name_to_instance = {name: f\"Individual{idx+1}\" for idx, name in enumerate(name_list)}\n",
        "\n",
        "    # 3b) Save an abstracted version of the ontology (optional)\n",
        "    abstracted_ontology_text = replace_all_ns1_names(ontology_text, name_to_instance)\n",
        "    with open(os.path.join(context_dir, entity + \".ttl\"), \"w\") as out_f:\n",
        "        out_f.write(abstracted_ontology_text)\n",
        "\n",
        "    # 3c) Now abstract each row's SPARQL Query and Answer\n",
        "    for _, row in entity_rows.iterrows():\n",
        "        #  - Answer: split on commas, map each via name_to_instance (if not found, keep original)\n",
        "        answer = str(row[\"Answer\"])\n",
        "        if pd.isna(answer) or answer.strip() == \"\":\n",
        "            DA_answer = \"\"\n",
        "        else:\n",
        "            parts = [item.strip() for item in answer.split(\",\")]\n",
        "            DA_answer = \", \".join(name_to_instance.get(p, p) for p in parts)\n",
        "\n",
        "        #  - SPARQL Query: replace every \"#name\" occurrence via name_to_instance\n",
        "        sparql = str(row.get(\"SPARQL Query\", \"\"))\n",
        "        if pd.isna(sparql) or sparql.strip() == \"\":\n",
        "            DA_sparql = \"\"\n",
        "        else:\n",
        "            DA_sparql = replace_in_sparql(sparql, name_to_instance)\n",
        "\n",
        "        new_DA_answers.append(DA_answer)\n",
        "        new_DA_sparqls.append(DA_sparql)\n",
        "\n",
        "# === Step 4: Attach new columns and save ===\n",
        "df[\"Abstracted Answer\"] = new_DA_answers\n",
        "df[\"Abstracted SPARQL Query\"] = new_DA_sparqls\n",
        "\n",
        "output_csv_path = \"/content/trial.csv\"\n",
        "df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "output_csv_path, context_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# === Step 5: Zip and download the Context folder ===\n",
        "context_zip_path = \"/content/Context.zip\"\n",
        "\n",
        "# Remove existing zip if re-running\n",
        "if os.path.exists(context_zip_path):\n",
        "    os.remove(context_zip_path)\n",
        "\n",
        "# Create a zip file from the context folder\n",
        "shutil.make_archive(base_name=\"/content/Context\", format='zip', root_dir=\"/content/Context\")\n",
        "\n",
        "# Download the zip file\n",
        "files.download(context_zip_path)\n"
      ],
      "metadata": {
        "id": "rTIYfjAVpeih",
        "outputId": "ccd8c94e-10eb-4830-8270-51aa33d740b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2ef78f5c-009e-4aa0-9035-b68001b5f64d\", \"Context.zip\", 86259)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}